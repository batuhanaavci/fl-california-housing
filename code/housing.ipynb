{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOWARDS DECENTRALIZED PRICING MODELS WITH FEDERATED LEARNING: A STUDY ON CALIFORNIA HOUSING DATASET\n",
    "Batuhan Avcı - 101629010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import networkx as nx\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses = pd.read_csv(f'housing.csv')\n",
    "houses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='longitude', y='latitude', data=houses, palette='coolwarm', legend=True)\n",
    "plt.title('Houses in California')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.grid(alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing values in total_bedroom column. Fill missing values by using median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses['total_bedrooms'].fillna(houses['total_bedrooms'].median(), inplace=True)\n",
    "houses.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the latitude and longitude columns, I would like to perform k-means clustering to split the data into 12 clusters. Each cluster will be used as a node in the graph. This process will add 3 columns to the data: area_index, central_lat, and central_lon. The area_index column will be used to identify the cluster that each data point belongs to. The central_lat and central_lon columns will be used to plot the nodes on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Clustering\n",
    "# Extract latitude and longitude\n",
    "lat_lon = houses[['latitude', 'longitude']]\n",
    "\n",
    "# Perform Agglomerative Hierarchical clustering to classify the places into different areas\n",
    "agg = AgglomerativeClustering(n_clusters=9)\n",
    "houses['area_index'] = agg.fit_predict(lat_lon)\n",
    "\n",
    "# Get the central coordinates for each area\n",
    "cluster_centers = []\n",
    "for i in range(9):\n",
    "    cluster_centers.append([\n",
    "        houses[houses['area_index'] == i]['latitude'].mean(),\n",
    "        houses[houses['area_index'] == i]['longitude'].mean()\n",
    "    ])\n",
    "\n",
    "# Map the central coordinates to the houses dataset\n",
    "houses['central_lat'] = houses['area_index'].apply(lambda x: cluster_centers[x][0])\n",
    "houses['central_lon'] = houses['area_index'].apply(lambda x: cluster_centers[x][1])\n",
    "\n",
    "# Display the updated houses dataset with the new columns\n",
    "houses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the data based on the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='longitude', y='latitude', data=houses, hue='area_index', palette='coolwarm', legend=True)\n",
    "plt.scatter([center[1] for center in cluster_centers], [center[0] for center in cluster_centers], color='black', s=100)\n",
    "plt.title('Clustered Areas in California (9 clusters with centers in black)', fontsize=15) \n",
    "plt.xlabel('Longitude', fontsize=16) \n",
    "plt.ylabel('Latitude', fontsize=16) \n",
    "plt.xticks(fontsize=12)  \n",
    "plt.yticks(fontsize=12) \n",
    "plt.grid(alpha=0.2)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histogram: the number of houses in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram number of houses in each area\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(houses['area_index'], bins=30, kde=True)\n",
    "plt.title('Number of Houses in Cluster')\n",
    "plt.xlabel('Area Index')\n",
    "plt.ylabel('Number of Houses')\n",
    "plt.grid(alpha=0.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of houses in each area\n",
    "houses['area_index'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column 'ocean_proximity' is a categorical column. I will convert it to a numerical column by using one-hot encoding. But first, let's visualize the data based on the 'ocean_proximity' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='longitude', y='latitude', data=houses, hue='ocean_proximity', palette='colorblind', legend=True)\n",
    "plt.title('Ocean Proximity For Each Data Point')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.grid(alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: One-Hot Encoding for 'ocean_proximity'\n",
    "houses_encoded = pd.get_dummies(houses, columns=['ocean_proximity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decide which columns to use for the model, I will check the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features for normalization including the one-hot encoded columns\n",
    "numerical_features_encoded = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
    "                              'total_bedrooms', 'population', 'households', 'median_income']\n",
    "\n",
    "all_features = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
    "                              'total_bedrooms', 'population', 'households',\n",
    "                                'median_income', 'ocean_proximity_<1H OCEAN',\n",
    "                                  'ocean_proximity_INLAND', 'ocean_proximity_ISLAND', \n",
    "                                  'ocean_proximity_NEAR BAY', 'ocean_proximity_NEAR OCEAN', \n",
    "                                  'area_index', 'central_lat', 'central_lon', 'median_house_value']\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = houses_encoded[all_features].corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the features by using min-max scaler. For the target variable, only scaling by 10000 will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide median_house_value by 1000 to scale it down\n",
    "houses_encoded['median_house_value'] /= 10000\n",
    "\n",
    "houses_normalized_encoded = houses_encoded.copy()\n",
    "\n",
    "# Features to be scaled for the whole dataset\n",
    "whole_dataset_features = ['area_index', 'central_lat', 'central_lon']\n",
    "\n",
    "# Apply Min-Max scaling to the specified features for the whole dataset\n",
    "scaler = MinMaxScaler()\n",
    "houses_normalized_encoded[whole_dataset_features] = scaler.fit_transform(houses_encoded[whole_dataset_features])\n",
    "\n",
    "# Function to apply Min-Max scaling to numerical features within each area_index group\n",
    "def scale_within_area(group):\n",
    "    scaler = MinMaxScaler()\n",
    "    group[numerical_features_encoded] = scaler.fit_transform(group[numerical_features_encoded])\n",
    "    return group\n",
    "\n",
    "# Apply the scaling function to each group based on area_index\n",
    "houses_normalized_encoded = houses_normalized_encoded.groupby('area_index').apply(scale_within_area)\n",
    "\n",
    "# Reset index if necessary\n",
    "houses_normalized_encoded.reset_index(drop=True, inplace=True)\n",
    "\n",
    "houses_normalized_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the target variable from the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the target column from the features\n",
    "X = houses_normalized_encoded.drop(columns='median_house_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotGraph(G_houses):    \n",
    "    coords = np.array([G_houses.nodes[node]['coord'] for node in G_houses.nodes])\n",
    "    \n",
    "    # Draw nodes\n",
    "    for node in G_houses.nodes:\n",
    "        plt.scatter(coords[node,1], coords[node,0], color='blue', s=50, zorder=5)  # zorder ensures nodes are on top of edges\n",
    "        plt.text(coords[node,1]+0.016, coords[node,0]+0.027, str(node), fontsize=8, ha='center', va='center', color='black', fontweight='bold')\n",
    "    \n",
    "    # Draw edges\n",
    "    for edge in G_houses.edges:\n",
    "        plt.plot([coords[edge[0],1],coords[edge[1],1]], [coords[edge[0],0],coords[edge[1],0]], linestyle='-', color='gray', alpha=0.5)\n",
    "\n",
    "    plt.xlabel('longitude')\n",
    "    plt.ylabel('latitude')\n",
    "    plt.title('Clustered Areas in California')\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# The function connects each clusterş with \n",
    "# the nearest neighbours. \n",
    "def add_edges(graph, numneighbors=2):\n",
    "    # Get the coordinates of the stations.\n",
    "    coords = np.array([G_houses.nodes[node]['coord'] for node in G_houses.nodes])\n",
    "    \n",
    "    A = kneighbors_graph(coords, numneighbors, mode='connectivity', include_self=False)\n",
    "    nrnodes = len(graph.nodes)\n",
    "    for iter_i in range(nrnodes): \n",
    "        for iter_ii in range(nrnodes): \n",
    "            if iter_i != iter_ii : \n",
    "                if A[iter_i,iter_ii]> 0 :\n",
    "                    graph.add_edge(iter_i, iter_ii)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def ExtractFeatureMatrixLabelVector(data):\n",
    "    nrfeatures = 16\n",
    "    nrdatapoints = len(data)\n",
    "    \n",
    "\n",
    "    X = np.zeros((nrdatapoints, nrfeatures))\n",
    "    y = np.zeros((nrdatapoints, 1))\n",
    "\n",
    "    for ind in range(nrdatapoints):\n",
    "        lat1 = float(data['latitude'].iloc[ind])\n",
    "        lon1 = float(data['longitude'].iloc[ind])\n",
    "        lat = float(data['central_lat'].iloc[ind])\n",
    "        lon = float(data['central_lon'].iloc[ind])\n",
    "        age = float(data['housing_median_age'].iloc[ind])\n",
    "        rooms = float(data['total_rooms'].iloc[ind])\n",
    "        bedrooms = float(data['total_bedrooms'].iloc[ind])\n",
    "        population = float(data['population'].iloc[ind])\n",
    "        income = float(data['median_income'].iloc[ind])\n",
    "        households = float(data['households'].iloc[ind])\n",
    "        area = float(data['area_index'].iloc[ind])\n",
    "        value = float(data['median_house_value'].iloc[ind])\n",
    "        ocean = float(data['ocean_proximity_<1H OCEAN'].iloc[ind])\n",
    "        inland = float(data['ocean_proximity_INLAND'].iloc[ind])\n",
    "        island = float(data['ocean_proximity_ISLAND'].iloc[ind])\n",
    "        nearbay = float(data['ocean_proximity_NEAR BAY'].iloc[ind])\n",
    "        nearocean = float(data['ocean_proximity_NEAR OCEAN'].iloc[ind])\n",
    "\n",
    "\n",
    "        X[ind,:] = [lat1, lon1, lat, lon, age, rooms, bedrooms, population, income, households, ocean, inland, area, island, nearbay, nearocean]\n",
    "        y[ind,:] = value\n",
    " \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 11\n",
    "num_areas = len(houses_normalized_encoded.area_index.unique())\n",
    "print(f'num_areas={num_areas}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ExtractFeatureMatrixLabelVector function is used to extract the feature matrix and label vector from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = ExtractFeatureMatrixLabelVector(houses_normalized_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "# Create a networkX graph\n",
    "G_houses = nx.Graph()\n",
    "\n",
    "# Add a one node per station\n",
    "G_houses.add_nodes_from(range(0, num_areas))\n",
    "\n",
    "for i, area in enumerate(houses_normalized_encoded.area_index.unique()):\n",
    "    # Extract data of a certain station\n",
    "    area_data = houses_normalized_encoded[houses_normalized_encoded.area_index==area]\n",
    "    \n",
    "    # Extract features and labels\n",
    "\n",
    "    X_local, y_local = ExtractFeatureMatrixLabelVector(area_data)\n",
    "    X_local.shape\n",
    "\n",
    "    #shuffle the data\n",
    "    X_local, y_local = shuffle(X_local, y_local, random_state=seed)\n",
    "    # Split the dataset into training and validation set, test\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_local, y_local, test_size=0.4, random_state=seed)\n",
    "    print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "    # Split validation set into validation and test set\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=5/8, random_state=seed)\n",
    "\n",
    "\n",
    "    # Stack local X_train X_val, y_train, y_val to compare FL model with a linear regression on whole dataset\n",
    "    if i == 0:\n",
    "        X_train_all = X_train\n",
    "        X_val_all = X_val\n",
    "        X_test_all = X_test\n",
    "        y_train_all = y_train\n",
    "        y_val_all = y_val\n",
    "        y_test_all = y_test\n",
    "    else:\n",
    "        X_train_all = np.vstack((X_train_all, X_train))\n",
    "        X_val_all = np.vstack((X_val_all, X_val))\n",
    "        X_test_all = np.vstack((X_test_all, X_test))\n",
    "        y_train_all = np.vstack((y_train_all, y_train))\n",
    "        y_val_all = np.vstack((y_val_all, y_val))\n",
    "        y_test_all = np.vstack((y_test_all, y_test))\n",
    "\n",
    "\n",
    "\n",
    "    # Create a linear regression model\n",
    "    local_samplesize = len(y_local)\n",
    "    G_houses.nodes[i]['samplesize'] = local_samplesize\n",
    "    G_houses.nodes[i]['coord'] = (area_data.central_lat.unique()[0], area_data.central_lon.unique()[0])\n",
    "    G_houses.nodes[i]['X_train'] = X_train # The training feature matrix for local dataset at node i\n",
    "    G_houses.nodes[i]['y_train'] = y_train  # The training label vector for local dataset at node i\n",
    "    G_houses.nodes[i]['X_val'] = X_val # The training feature matrix for local dataset at node i\n",
    "    G_houses.nodes[i]['y_val'] = y_val  # The training label vector for local dataset at node i\n",
    "    G_houses.nodes[i]['X_test'] = X_test # The training feature matrix for local dataset at node i\n",
    "    G_houses.nodes[i]['y_test'] = y_test  # The training label vector for local dataset at node i\n",
    "    G_houses.nodes[i]['weights'] = np.zeros((16, 1)) # The weight vector for local dataset at node i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Benchmark Model\n",
    "I will use the linear regression model on the whole data as the benchmark model. I want to see how well the FL algorithms performs compared to the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform linear regression on whole dataset\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_all, y_train_all)\n",
    "y_train_pred = model.predict(X_train_all)\n",
    "y_val_pred = model.predict(X_val_all)\n",
    "y_test_pred = model.predict(X_test_all)\n",
    "\n",
    "mse_train = mean_squared_error(y_train_all, y_train_pred)\n",
    "mse_val = mean_squared_error(y_val_all, y_val_pred)\n",
    "mse_test = mean_squared_error(y_test_all, y_test_pred)\n",
    "\n",
    "print(f'Mean Squared Error on Training Set: {mse_train:.2f}')\n",
    "print(f'Mean Squared Error on Validation Set: {mse_val:.2f}')\n",
    "print(f'Mean Squared Error on Test Set: {mse_test:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FedGD Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FedGD(G_houses, alpha, learning_rate, max_iter=1000):\n",
    "    G_houses_gd = G_houses.copy()\n",
    "    num_areas = len(G_houses.nodes)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # Iterate over all nodes.\n",
    "        for current_node in G_houses_gd.nodes:\n",
    "            # Extract the training data from the current node.\n",
    "            X_train = G_houses_gd.nodes[current_node]['X_train']\n",
    "            y_train = G_houses_gd.nodes[current_node]['y_train']\n",
    "            w_current = G_houses_gd.nodes[current_node]['weights']\n",
    "            training_size = len(y_train)\n",
    "\n",
    "            # Compute the first term of the Equation 5.9.\n",
    "            term_1 = (2 / training_size) * X_train.T.dot(y_train - X_train.dot(w_current))\n",
    "            # Compute the second term of the Equation 5.9\n",
    "            # by receiving neighbors' weight vectors.\n",
    "            term_2 = 0\n",
    "            neighbors = list(G_houses_gd.neighbors(current_node))\n",
    "            for neighbor in neighbors:\n",
    "                w_neighbor = G_houses_gd.nodes[neighbor]['weights']\n",
    "                term_2 += w_neighbor - w_current\n",
    "            term_2 *= 2 * alpha\n",
    "            # Equation 5.9\n",
    "            w_updated = w_current + learning_rate * (term_1 + term_2)\n",
    "\n",
    "            # Update the current weight vector but do not overwrite the\n",
    "            # \"weights\" attribute as we need to do all updates synchronously, i.e.,\n",
    "            # using the previous local params\n",
    "            G_houses_gd.nodes[current_node]['newweights'] = w_updated\n",
    "\n",
    "        # After computing the new localparmas for each node, we now update\n",
    "        # the node attribute 'weights' for all nodes\n",
    "        for node_id in G_houses_gd.nodes:\n",
    "            G_houses_gd.nodes[node_id]['weights'] = G_houses_gd.nodes[node_id]['newweights']\n",
    "\n",
    "    # Create the storages for the training and validation errors.\n",
    "    train_errors = np.zeros(num_areas)\n",
    "    val_errors = np.zeros(num_areas)\n",
    "\n",
    "    # Iterate over all nodes.\n",
    "    for station in G_houses_gd.nodes:\n",
    "        # Extract the data of the current node.\n",
    "        X_train = G_houses_gd.nodes[station]['X_train']\n",
    "        y_train = G_houses_gd.nodes[station]['y_train']\n",
    "        X_val = G_houses_gd.nodes[station]['X_val']\n",
    "        y_val = G_houses_gd.nodes[station]['y_val']\n",
    "        w = G_houses_gd.nodes[station]['weights']\n",
    "\n",
    "        # Compute and store the training and validation errors.\n",
    "        train_errors[station] = mean_squared_error(y_train, X_train.dot(w))\n",
    "        val_errors[station] = mean_squared_error(y_val, X_val.dot(w))\n",
    "\n",
    "    # Output the average training and validation errors.\n",
    "    avg_train_error = np.mean(train_errors)\n",
    "    avg_val_error = np.mean(val_errors)\n",
    "    \n",
    "    return avg_train_error, avg_val_error, G_houses_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfrom grid search to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search loop\n",
    "num_neighbours_list = [3, 4, 5]\n",
    "alpha_list = [0.5, 0.2, 0.1]\n",
    "learning_rate_list = [0.1, 0.05, 0.01]\n",
    "\n",
    "\n",
    "\n",
    "# Create a storage for the results\n",
    "results = []\n",
    "\n",
    "# Loop over all combinations of hyperparameters\n",
    "for num_neighbours in num_neighbours_list:\n",
    "    G_houses_c = G_houses.copy()\n",
    "    G_houses_c = add_edges(G_houses_c, numneighbors=num_neighbours)\n",
    "    print(\"\\nThe empirical graph is connected:\", nx.is_connected(G_houses_c))\n",
    "\n",
    "    for alpha in alpha_list:\n",
    "        for learning_rate in learning_rate_list:\n",
    "\n",
    "            avg_train_error, avg_val_error, trained_graph = FedGD(G_houses_c, alpha, learning_rate)\n",
    "\n",
    "            # Output the average training and validation errors.\n",
    "            print(f\"num_neighbours: {num_neighbours}, alpha: {alpha}, learning_rate: {learning_rate}, avg_train_error: {avg_train_error}, avg_val_error: {avg_val_error}\")\n",
    "\n",
    "            # Store the results\n",
    "            results.append((num_neighbours, alpha, learning_rate, avg_train_error, avg_val_error, trained_graph))\n",
    "\n",
    "# Sort results based on the average validation error\n",
    "results.sort(key=lambda x: x[4])\n",
    "print(\"\\nBest hyperparameters found:\")\n",
    "best_params_gd = results[0]\n",
    "print(best_params_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the best hyperparameters (used validation set), evaluate the model on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num_neighbours_gd, best_alpha_gd, best_learning_rate_gd, _, _, best_trained_graph_gd = best_params_gd\n",
    "plotGraph(best_trained_graph_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the storages for the test errors.\n",
    "test_errors = np.zeros(num_areas)\n",
    "train_errors = np.zeros(num_areas)\n",
    "val_errors = np.zeros(num_areas)\n",
    "\n",
    "# Iterate over all nodes.\n",
    "for station in best_trained_graph_gd.nodes:\n",
    "    # Extract the data of the current node.\n",
    "    X_test = best_trained_graph_gd.nodes[station]['X_test']\n",
    "    y_test = best_trained_graph_gd.nodes[station]['y_test']\n",
    "    X_train = best_trained_graph_gd.nodes[station]['X_train']\n",
    "    y_train = best_trained_graph_gd.nodes[station]['y_train']\n",
    "    X_val = best_trained_graph_gd.nodes[station]['X_val']\n",
    "    y_val = best_trained_graph_gd.nodes[station]['y_val']\n",
    "\n",
    "    w = best_trained_graph_gd.nodes[station]['weights']\n",
    "\n",
    "    # Compute and store the test errors.\n",
    "    test_errors[station] = mean_squared_error(y_test, X_test.dot(w))\n",
    "    train_errors[station] = mean_squared_error(y_train, X_train.dot(w))\n",
    "    val_errors[station] = mean_squared_error(y_val, X_val.dot(w))\n",
    "    print(\"For node \", station)\n",
    "    print(\"-------------------\")\n",
    "\n",
    "    print(f\"Train error for node {station}: {train_errors[station]}\")\n",
    "    print(f\"Validation error for node {station}: {val_errors[station]}\")\n",
    "    print(f\"Test error for node {station}: {test_errors[station]}\")\n",
    "\n",
    "# Output the average test error.\n",
    "avg_test_error = np.mean(test_errors)\n",
    "print(\"\\nRecall: The average training error:\", best_params_gd[3])\n",
    "print(\"Recall: The average validation error:\", best_params_gd[4])\n",
    "print(\"The average test error:\", avg_test_error)\n",
    "\n",
    "#mean squared error on the train set\n",
    "print(f'Mean Squared Error on Training Set: {np.mean(train_errors):.2f}')\n",
    "#mean squared error on the validation set\n",
    "print(f'Mean Squared Error on Validation Set: {np.mean(val_errors):.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the prediction results. For the node 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 3\n",
    "\n",
    "y_val = best_trained_graph_gd.nodes[ind]['y_test']\n",
    "y_pred = best_trained_graph_gd.nodes[ind]['X_test'].dot(best_trained_graph_gd.nodes[ind]['weights'])\n",
    "\n",
    "# Select a subset of data points for better visualization \n",
    "subset_size = min(100, len(y_val))  # Use up to 100 points or less if the dataset is smaller\n",
    "indices = np.arange(subset_size)\n",
    "y_val_subset = y_val[:subset_size]\n",
    "y_pred_subset = y_pred[:subset_size]\n",
    "\n",
    "# Calculate absolute errors\n",
    "errors = np.abs(y_val_subset - y_pred_subset)\n",
    "\n",
    "# Plot true vs predicted values\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(indices, y_val_subset, label='True values', marker='o')\n",
    "plt.plot(indices, y_pred_subset, label='Predicted values', marker='x')\n",
    "plt.xlabel('Data points')\n",
    "plt.ylabel('Price (in $10000)')\n",
    "plt.title('True and Predicted Values FedGD')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FedSGD Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FedSGD(G_houses, alpha, learning_rate, batch_size, max_iter=1000):\n",
    "    G_houses_sgd = G_houses.copy()\n",
    "    num_areas = len(G_houses.nodes)\n",
    "    \n",
    "    for station in G_houses_sgd.nodes:\n",
    "        G_houses_sgd.nodes[station]['weights'] = np.zeros((G_houses_sgd.nodes[station]['X_train'].shape[1], 1))\n",
    "        G_houses_sgd.nodes[station]['curr_batch_start'] = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Iterate over all nodes.\n",
    "        for current_node in G_houses_sgd.nodes:\n",
    "            # Extract the training data from the current node.\n",
    "            X_train = G_houses_sgd.nodes[current_node]['X_train']\n",
    "            y_train = G_houses_sgd.nodes[current_node]['y_train']\n",
    "            w_current = G_houses_sgd.nodes[current_node]['weights']\n",
    "            training_size = len(y_train)\n",
    "\n",
    "            # Compute the first term of the Equation 5.11.\n",
    "            curr_batch_start = G_houses_sgd.nodes[current_node]['curr_batch_start']\n",
    "            # Get the batched features and labels\n",
    "            X_train_batch = X_train[curr_batch_start:(curr_batch_start+batch_size)]\n",
    "            y_train_batch = y_train[curr_batch_start:(curr_batch_start+batch_size)]\n",
    "            \n",
    "            # update batch start for the next iteration\n",
    "            curr_batch_start = curr_batch_start + batch_size\n",
    "            # check if batch start would be outside the training set \n",
    "            if curr_batch_start >= training_size: \n",
    "                curr_batch_start = 0  # if next batch exceeds training set size start over from first datapoint\n",
    "            G_houses_sgd.nodes[current_node]['curr_batch_start'] = curr_batch_start\n",
    "            \n",
    "            term_1 = (2 / batch_size) * X_train_batch.T.dot(y_train_batch - X_train_batch.dot(w_current))\n",
    "            \n",
    "            # Compute the second term of the Equation 5.11\n",
    "            # by receiving neighbors' weight vectors.\n",
    "            term_2 = 0\n",
    "            neighbors = list(G_houses_sgd.neighbors(current_node))\n",
    "            for neighbor in neighbors:\n",
    "                w_neighbor = G_houses_sgd.nodes[neighbor]['weights']\n",
    "                term_2 += w_neighbor - w_current\n",
    "            term_2 *= 2 * alpha\n",
    "            # Equation 5.11\n",
    "            w_updated = w_current + learning_rate * (term_1 + term_2)\n",
    "            \n",
    "            # Update the current weight vector but do not overwrite the \n",
    "            # \"weights\" attribute as we need to do all updates synchronously, i.e., \n",
    "            # using the previous local params \n",
    "            G_houses_sgd.nodes[current_node]['newweights'] = w_updated\n",
    "        \n",
    "        # After computing the new localparmas for each node, we now update \n",
    "        # the node attribute 'weights' for all nodes \n",
    "        for node_id in G_houses_sgd.nodes: \n",
    "            G_houses_sgd.nodes[node_id]['weights'] = G_houses_sgd.nodes[node_id]['newweights']\n",
    "\n",
    "    # Create the storages for the training and validation errors.\n",
    "    train_errors = np.zeros(num_areas)\n",
    "    val_errors = np.zeros(num_areas)\n",
    "\n",
    "    # Iterate over all nodes.\n",
    "    for station in G_houses_sgd.nodes:\n",
    "        # Extract the data of the current node.\n",
    "        X_train = G_houses_sgd.nodes[station]['X_train']\n",
    "        y_train = G_houses_sgd.nodes[station]['y_train']\n",
    "        X_val = G_houses_sgd.nodes[station]['X_val']\n",
    "        y_val = G_houses_sgd.nodes[station]['y_val']\n",
    "        w = G_houses_sgd.nodes[station]['weights']\n",
    "\n",
    "        # Compute and store the training and validation errors.\n",
    "        train_errors[station] = mean_squared_error(y_train, X_train.dot(w))\n",
    "        val_errors[station] = mean_squared_error(y_val, X_val.dot(w))\n",
    "\n",
    "    # Output the average training and validation errors.\n",
    "    avg_train_error = np.mean(train_errors)\n",
    "    avg_val_error = np.mean(val_errors)\n",
    "    \n",
    "    return avg_train_error, avg_val_error, G_houses_sgd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfrom grid search to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters grid\n",
    "num_neighbours_list = [3, 4, 5]\n",
    "alpha_list = [0.5, 0.2, 0.1]\n",
    "learning_rate_list = [0.1, 0.05, 0.01]\n",
    "batch_size_list = [16, 32]\n",
    "\n",
    "# Create a storage for the results\n",
    "results = []\n",
    "\n",
    "# Loop over all combinations of hyperparameters\n",
    "for num_neighbours in num_neighbours_list:\n",
    "    G_houses_c2 = G_houses.copy()\n",
    "    G_houses_c2 = add_edges(G_houses_c2, numneighbors=num_neighbours)\n",
    "    print(\"\\nThe empirical graph is connected:\", nx.is_connected(G_houses_c2))\n",
    "\n",
    "    for alpha in alpha_list:\n",
    "        for learning_rate in learning_rate_list:\n",
    "            for batch_size in batch_size_list:\n",
    "                avg_train_error, avg_val_error, trained_graph = FedSGD(G_houses_c2, alpha, learning_rate, batch_size)\n",
    "                print(f\"num_neighbours: {num_neighbours}, alpha: {alpha}, learning_rate: {learning_rate}, batch_size: {batch_size}, avg_train_error: {avg_train_error}, avg_val_error: {avg_val_error}\")\n",
    "                results.append((num_neighbours, alpha, learning_rate, batch_size, avg_train_error, avg_val_error, trained_graph))\n",
    "\n",
    "# Sort results based on the average validation error\n",
    "results.sort(key=lambda x: x[5])\n",
    "print(\"\\nBest hyperparameters found:\")\n",
    "best_params_sgd = results[0]\n",
    "print('best_num_neighbours:', best_params_sgd[0], 'best_alpha:', best_params_sgd[1], 'best_learning_rate:', best_params_sgd[2], 'best_batch_size:', best_params_sgd[3], 'avg_train_error:', best_params_sgd[4], 'avg_val_error:', best_params_sgd[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the best hyperparameters (used validation set), evaluate the model on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num_neighbours_sgd, best_alpha_sgd, best_learning_rate_sgd, best_batch_size_sgd, _, _, best_trained_graph_sgd = best_params_sgd\n",
    "plotGraph(best_trained_graph_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the storages for the test errors.\n",
    "test_errors = np.zeros(num_areas)\n",
    "train_errors = np.zeros(num_areas)\n",
    "val_errors = np.zeros(num_areas)\n",
    "\n",
    "# Iterate over all nodes.\n",
    "for station in best_trained_graph_sgd.nodes:\n",
    "    # Extract the data of the current node.\n",
    "    X_test = best_trained_graph_sgd.nodes[station]['X_test']\n",
    "    y_test = best_trained_graph_sgd.nodes[station]['y_test']\n",
    "    X_train = best_trained_graph_sgd.nodes[station]['X_train']\n",
    "    y_train = best_trained_graph_gd.nodes[station]['y_train']\n",
    "    X_val = best_trained_graph_sgd.nodes[station]['X_val']\n",
    "    y_val = best_trained_graph_sgd.nodes[station]['y_val']\n",
    "\n",
    "    w = best_trained_graph_sgd.nodes[station]['weights']\n",
    "\n",
    "    # Compute and store the test errors.\n",
    "    test_errors[station] = mean_squared_error(y_test, X_test.dot(w))\n",
    "    train_errors[station] = mean_squared_error(y_train, X_train.dot(w))\n",
    "    val_errors[station] = mean_squared_error(y_val, X_val.dot(w))\n",
    "    print(\"For node \", station)\n",
    "    print(\"-------------------\")\n",
    "\n",
    "    print(f\"Train error for node {station}: {train_errors[station]}\")\n",
    "    print(f\"Validation error for node {station}: {val_errors[station]}\")\n",
    "    print(f\"Test error for node {station}: {test_errors[station]}\")\n",
    "\n",
    "# Output the average test error.\n",
    "avg_test_error = np.mean(test_errors)\n",
    "print(\"\\nRecall: The average training error:\", best_params_sgd[4])\n",
    "print(\"Recall: The average validation error:\", best_params_sgd[5])\n",
    "print(\"The average test error:\", avg_test_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 3\n",
    "y_val = best_trained_graph_sgd.nodes[ind]['y_test']\n",
    "y_pred = best_trained_graph_sgd.nodes[ind]['X_test'].dot(best_trained_graph_sgd.nodes[ind]['weights'])\n",
    "\n",
    "# Select a subset of data points for better visualization\n",
    "subset_size = min(100, len(y_val))  # Use up to 100 points or less if the dataset is smaller\n",
    "indices = np.arange(subset_size)\n",
    "y_val_subset = y_val[:subset_size]\n",
    "y_pred_subset = y_pred[:subset_size]\n",
    "\n",
    "# Calculate absolute errors\n",
    "errors = np.abs(y_val_subset - y_pred_subset)\n",
    "\n",
    "# Plot true vs predicted values\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(indices, y_val_subset, label='True values', marker='o')\n",
    "plt.plot(indices, y_pred_subset, label='Predicted values', marker='x')\n",
    "plt.xlabel('Data points')\n",
    "plt.ylabel('Price (in $10000)')\n",
    "plt.title('True and Predicted Values FedSGD')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.2)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "A. Jung, “Lecture Notes for CS-E4740 Federated Learning,”, available at https://github.com/alexjungaalto/FederatedLearning/blob/main/material/FL_LectureNotes.pdf, 2024.\n",
    "\n",
    "CS-E4740 Federated Learning Course Assignments: https://github.com/alexjungaalto/FederatedLearning/tree/main/material/Assignments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
